给出了四个FL标准假设，首先第一个函数是说明全局目标函数无论怎么优化，都不会无限减少，总会有一个下界Finf,第二行说明满足Lipschirz性质的函数更容易优化，并且函数的变化有限，没有剧烈的波峰，
第三个和第四个函数保证了全局目标函数和客户端的目标函数在优化过程中受到某些因素的限制，比如我们这里所需的dk，也就是上面说的全局和局部的差异性。然后它结合这四个假设，证明了全局目标函数优化过程的误差上界，客户端在每次迭代过程中，当前解与最优的解总是存在误差，然后这个函数就是说明这个误差有个上界，并且这个上界与这么多因素有关，比如n(局部学习率），下界Finf,步长等等，还有这个权重Pk和差异性dk,因为误差肯定是尽量减小，所以这个误差上界也尽量减小，T1，T2,T3,T4尽量变小，T0尽量变大，为了解决以下优化问题，出了权重pk与数据集大小nk和差异性的关系dk